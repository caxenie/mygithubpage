<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>Applied AI and ML Applications - Dr. Cristian Axenie </title>
    <meta name="description" content="Applied AI and ML Applications: VR, Robotics, Big Data.">
  </head>

  <body>
    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <h1>
<a id="welcome-to-my-sandbox" class="anchor" href="#welcome-to-my-sandbox" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Personal page: Cristian Axenie</strong>
</h1>

<h2>
<a id="a-messy-place-to-store-opinions-work-and-all-sorts-of-ideas-on-AI-neuroscience-neural-engineering-big-data-robotics-and-coding" class="anchor" href="#a-messy-place-to-store-opinions-work-and-all-sorts-of-ideas-on-neuroscience-neural-engineering-robotics-and-coding" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bio</h2>

<p> After earning a PhD in Neuroscience and Robotics from TUM in 2016, I've spent one more year with the TUM Center of Competence Neuroengineering before joining Huawei Research Center in Munich. Since 2017 I am Senior AI and ML Research Engineer with Huawei's largest research center outside China. At the same time, I lead the Audi Konfuzius-Institut Ingolstadt Lab, a Sino-German research initiative focused on combining modern AI and VR technology for applications ranging from sports technology to biomedical engineering and medical rehabilitation. Each term, I teach AI and ML for undergrads in TH Ingolstadt.</p>

<script type="text/javascript" src="https://platform.linkedin.com/badges/js/profile.js" async defer></script>
<div class="LI-profile-badge"  data-version="v1" data-size="medium" data-locale="en_US" data-type="horizontal" data-theme="light" data-vanity="caxenie"><a class="LI-simple-link" href='https://de.linkedin.com/in/caxenie?trk=profile-badge'>Cristian Axenie</a></div>
<p style="clear: both;">	
<p> A recent curriculum vitae:</p>
<p>(<a href="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/master/CV_Axenie_2020_extended_web.pdf">Extended Curriculum Vitae</a>)</p>

<h2>		
<a id="slectedpub" class="anchor" href="#selectedpub" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Selected Publications</strong>
</h2>	
<script src="https://bibbase.org/show?bib=https%3A%2F%2Fgithub.com%2Fcaxenie%2Fneurorobotics.me_backend%2Fraw%2Fmaster%2Fpublications_axenie.bib&jsonp=1"></script>	
		
<hr>

<h2>
<a id="research" class="anchor" href="#research" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Research</strong>
</h2>

<h3>GLUECK: Growth pattern Learning for Unsupervised Extraction of Cancer Kinetics</h3>
Neoplastic processes are described by complex and heterogeneous dynamics. The interaction of neoplastic cells with their environment describes tumor growth and is critical for the initiation of cancer invasion. Despite the large spectrum of tumor growth models, there is no clear guidance on how to choose the most appropriate model for a particular cancer and how this will impact its subsequent use in therapy planning. Such models need parametrization that is dependent on tumor biology and hardly generalize to other tumor types and their variability. Moreover, the datasets are small in size due to the limited or expensive measurement methods. Alleviating the limitations that incomplete biological descriptions, the diversity of tumor types, and the small size of the data bring to mechanistic models, we introduce Growth pattern Learning for Unsupervised Extraction of Cancer Kinetics (GLUECK) a novel, data-driven model based on a neural network capable of unsupervised learning of cancer growth curves. Employing mechanisms of competition, cooperation, and correlation in neural networks, GLUECK learns the temporal evolution of the input data along with the underlying distribution of the input space. We demonstrate the superior accuracy of GLUECK, against four typically used tumor growth models, in extracting growth curves from a set of four clinical tumor datasets. Our experiments show that, without any modification, GLUECK can learn the underlying growth curves being versatile between and within tumor types.

<a href="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2020/06/fig6.png"><img class="size-large wp-image-26698 aligncenter" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2020/06/fig6-1024x437.png" alt="" width="1024" height="437" /></a>

<strong>Code</strong>

<a href="https://gitlab.com/akii-microlab/ecml-2020-glueck-codebase">https://gitlab.com/akii-microlab/ecml-2020-glueck-codebase</a>

<hr />

<h3>PRINCESS: Prediction of Individual Breast Cancer Evolution to Surgical Size</h3>
Modelling surgical size is not inherently meant to replicate the tumor's exact form and proportions, but instead to elucidate the degree of the tissue volume that may be surgically removed in terms of improving patient survival and minimize the risk that a second or third operation will be needed to eliminate all malignant cells entirely. Given the broad range of models of tumor growth, there is no specific rule of thumb about how to select the most suitable model for a particular breast cancer type and whether that would influence its subsequent application in surgery planning. Typically, these models require tumor biology-dependent parametrization, which hardly generalizes to cope with tumor heterogeneity. In addition, the datasets are limited in size owing to the restricted or expensive methods of measurement. We address the shortcomings that incomplete biological specifications, the variety of tumor types and the limited size of the data bring to existing mechanistic tumor growth models and introduce a Machine Learning model for the PRediction of INdividual breast Cancer Evolution to Surgical Size (PRINCESS). This is a data-driven model based on neural networks capable of unsupervised learning of cancer growth curves. PRINCESS learns the temporal evolution of the tumor along with the underlying distribution of the measurement space. We demonstrate the superior accuracy of PRINCESS, against four typically used tumor growth models, in extracting tumor growth curves from a set of nine clinical breast cancer datasets. Our experiments show that, PRINCESS can learn the underlying growth curves being versatile between breast cancer types.

<a href="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2020/06/fig6-1.png"><img class="size-full wp-image-26701 aligncenter" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2020/06/fig6-1.png" alt="" width="549" height="618" /></a>


<strong>Code</strong>

<a href="https://gitlab.com/akii-microlab/cbms2020">https://gitlab.com/akii-microlab/cbms2020 </a>

<hr />

<div>
	
<h3>TUCANN: TUmor Characterization using Artificial Neural Networks</h3>
Despite the variety of imaging, genetic and histopathological data used to assess tumors, there is still an unmet need for patient-specific tumor growth profile extraction and tumor volume prediction, for use in surgery planning. Models of tumor growth predict tumor size based on measurements made in histological images of individual patients’ tumors compared to diagnostic imaging. Typically, such models require tumor biology-dependent parametrization, which hardly generalizes to cope with tumor variability among patients. In addition, the histopathology specimens datasets are limited in size, owing to the restricted or single-time measurements. In this work, we address the shortcomings that incomplete biological specifications, the inter-patient variability of tumors, and the limited size of the data bring to mechanistic tumor growth models and introduce a machine learning model capable of characterizing a tumor, namely its growth pattern, phenotypical transitions, and volume. The model learns without supervision, from different types of breast cancer data the underlying mathematical relations describing tumor growth curves more accurate than three state-of-the-art models on three publicly available clinical breast cancer datasets, being versatile among breast cancer types. Moreover, the model can also, without modification, learn the mathematical relations among, for instance, histopathological and morphological parameters of the tumor and together with the growth curve capture the (phenotypical) growth transitions of the tumor from a small amount of data. Finally, given the tumor growth curve and its transitions, our model can learn the relation among tumor proliferation-to-apoptosis ratio, tumor radius, and tumor nutrient diffusion length to estimate tumor volume, which can be readily incorporated within current clinical practice, for surgery planning. We demonstrate the broad capabilities of our model through a series of experiments on publicly available clinical datasets.

<a href="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2020/06/fig6b.png"><img class="size-large wp-image-26704 aligncenter" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2020/06/fig6b-1024x433.png" alt="" width="1024" height="433" /></a>


<strong>Preprint</strong>

<a href="https://www.biorxiv.org/content/10.1101/2020.06.08.140723v1">https://www.biorxiv.org/content/10.1101/2020.06.08.140723v1  </a>


<strong>Code</strong>

<a href="https://gitlab.com/akii-microlab/icann-2020-bio">https://gitlab.com/akii-microlab/icann-2020-bio </a>

</div>

<hr />

<h3>CHIMERA: Combining Mechanistic Models and Machine Learning for Personalized Chemotherapy and Surgery Sequencing in Breast Cancer</h3>
Mathematical and computational oncology has increased the pace of cancer research towards the advancement of personalized therapy. Serving the pressing need to exploit the large amounts of currently underutilized data, such approaches bring a significant clinical advantage in tailoring the therapy. CHIMERA is a novel system that combines mechanistic modelling and machine learning for personalized chemotherapy and surgery sequencing in breast cancer. It optimizes decision-making in personalized breast cancer therapy by connecting tumor growth behaviour and chemotherapy effects through predictive modelling and learning. We demonstrate the capabilities of CHIMERA in learning simultaneously the tumor growth patterns, across several types of breast cancer, and the pharmacokinetics of a typical breast cancer chemotoxic drug. The learnt functions are subsequently used to predict how to sequence the intervention. We demonstrate the versatility of CHIMERA in learning from tumor growth and pharmacokinetics data to provide robust predictions under two, typically used, chemotherapy protocol hypotheses.

<a href="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2020/06/fig2a.png"><img class="size-large wp-image-26707 aligncenter" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2020/06/fig2a-1024x722.png" alt="" width="1024" height="722" /></a>

<strong>Preprint</strong>

<a href="https://www.biorxiv.org/content/10.1101/2020.06.08.140756v1">https://www.biorxiv.org/content/10.1101/2020.06.08.140756v1 </a>

		
<strong>Code</strong>

<a href="https://gitlab.com/akii-microlab/bibe2020">https://gitlab.com/akii-microlab/bibe2020 </a>
		
<hr>
		
<h3>
<a id="virtooair" class="anchor" href="#virtooair" aria-hidden="true"><span class="octicon octicon-link"></span></a>VIRTOOAIR: VIrtual Reality TOOlbox for Avatar Intelligent Reconstruction</h3>

<p> 
The project focuses on designing and developing a Deep Learning framework for improved avatar representations in immersive collaborative virtual environments. The proposed infrastructure will be built on a modular architecture tackling: a) a predictive avatar tracking module; b) an inverse kinematic learning module; c) an efficient data representation and compression module.
</p>
		
<p>
In order to perform precise predictive tracking of the body without using a camera motion capture system we need proper calibration data of the 18 degrees-of-freedom provided by the VR devices, namely the HMD and the two hand controllers. Such a calibration procedure involves the mathematical modelling of a complex geometrical setup. As a first component of VIRTOOAIR we propose a novel position calibration method using deep artificial neural networks, as depicted in the next figure.
</p>
		
<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/caxenie-patch-1/pics/neural_calibration-768x466.png" width="80%" height="80%" alt=""></p>
	
<p> 
The second component in the VIRTOOAIR toolbox is the inverse kinematics learner, generically described in the following diagram. The problem of learning of inverse kinematics in VR avatars interactions is useful when the kinematics of the head, body or controllers are not accurately available, when Cartesian information is not available from camera coordinates, or when the computation complexity of analytical solutions becomes too high.
</p>
		
<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/caxenie-patch-1/pics/kinematic_learner-700x506.png" width="80%" height="80%" alt=""></p>

<p> 
Data and bandwidth constraints are substantial in remote VR environments. However, such problems can be solved through compression techniques and network topologies advances. VIRTOOAIR proposes to tackle this problem through its third component, a neural network data representation (compression and reconstruction) module, described in the following diagram.
</p>
		
<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/caxenie-patch-1/pics/data_compressor-768x447.png" width="80%" height="80%" alt=""></p>
	
<p>
PRELIMINARY RESULTS
</p>

<p>
VR MOTION RECONSTRUCTION BASED ON A VR TRACKING SYSTEM AND A SINGLE RGB CAMERA
</p>
<p>
Our preliminary results demonstrate the advantages of our system’s avatar pose reconstruction. This is mainly determined by the use of a powerful learning system, which offers significantly better results than existing heuristic solutions for inverse kinematics. Our system supports the paradigm shift towards learning systems capable to track full-body avatars inside Virtual Reality without the need of expensive external tracking hardware. The following figure shows preliminary results of our proposed reconstruction system.
</p>
		
<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/caxenie-patch-1/pics/recon1.png" width="80%" height="80%" alt=""></p>

<p>
For the upper body reconstruction, the semantically higher VR tracking system data is used. The lower body parts are reconstructed using state-of-the-art deep learning mechanism which allows pose recovery in an end-to-end manner. The system can be split in five different parts: tracking data acquisition, inverse kinematic regression, image processing, end-to-end recovery and visualization.	
</p>
		
<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/caxenie-patch-1/pics/recon2.png" width="80%" height="80%" alt=""></p>
		
<h3>		
<a id="publications" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Publications</h3>

<h3>
<a id="peer-reviewed-journal-papers" class="anchor" href="#peer-reviewed-journal-papers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Peer reviewed papers</h3>

<p>A. Becher, C. Axenie, T. Grauschopf, VIRTOOAIR: VIrtual Reality TOOlbox for Avatar Intelligent Reconstruction,  2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR2018).</p>
	    
<h3>
<a id="online-ml" class="anchor" href="#online-ml" aria-hidden="true"><span class="octicon octicon-link"></span></a>Online Distributed Machine Learning on Streams</h3>

<p> 
Streams are sequences of events (i.e. tuples containing various types of data) that are generated by various sources (e.g. sensors, machines or humans) in a chronologically ordered fashion. The stream processing paradigm involves applying business functions over the events in the stream. A typical approach to stream processing assumes accumulating such events within certain boundaries at a given time and applying functions on the resulting collection. Such transient event collections are termed windows.
</p>

<p>	      
Stream processing paradigm simplifies parallel software and hardware by restricting the parallel computation that can be performed. 
Given a sequence of data (a stream), a series of operations (functions) is applied to each element in the stream, in a declarative way, we specify what we want to achieve and not how.
</p>
<p>	      
Big Data Stream Online Learning is more challenging than batch or offline learning, since the data may not preserve the same distribution over the lifetime of the stream. 
Moreover, each example coming in a stream can only be processed once, or needs to be summarized with a small memory footprint, and the learning algorithms must be efficient.
</p>
<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/caxenie-patch-1/pics/online-ml.png" width="80%" height="80%" alt=""></p>
<p>		
The need for Online Machine Learning 
</p>
<p>
How to compute the entropy of a collection of infinite data, where the domain of the variables can be huge and the number of classes of objects is not known a priori?
How to maintain the k-most frequent items in a retail data warehouse with 3 TB of data, 100s of GB of new sales records updated daily with 1000000s different items?
What becomes of statistical computations when the learner can only afford one pass through each data sample because of time and memory constraints; when the learner has to decide on-the-fly what is relevant and process it and what is redundant and could be discarded?
</p>
<p>	      
This project focuses on developing new Online Machine Learning algorithms that run distributedly on clusters using a stream processor.	

The characteristics of the streaming data entail a new vision due to the fact that:
- Data are made available through unlimited streams that continuously flow, eventually at high speed, over time;
- The underlying regularities may evolve over time rather than being stationary;
- The data can no longer be considered as independent and identically distributed;
- The data are now often spatially as well as time situated.
</p>
		
<h3>
<a id="publications" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Publications</h3>

<h3>
<a id="peer-reviewed-journal-papers" class="anchor" href="#peer-reviewed-journal-papers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Peer reviewed papers</h3>

<p>C. Axenie et al., STARLORD: Sliding window Temporal Accumulate-Retract Learning for Online Reasoning on Datastreams, 2018 IEEE International Conference on Machine Learning and Applications (ICMLA2018). (submitted) </p>

<p>D. Foroni, C. Axenie et al., Moira: A Goal-Oriented Incremental Machine Learning Approach to Dynamic Resource Cost Estimation in Distributed Stream Processing Systems, International Workshop on Real-Time BI and Analytics, VLDB 2018. </p>
		
		
<h3>
<a id="deep learning autonomous systems" class="anchor" href="#deep-learning-autonomous-systems" aria-hidden="true"><span class="octicon octicon-link"></span></a>Deep Learning for Autonomous Systems </h3>

<p>
Scope
</p>
<p>
The current research project aims at exploring object detection algorithms using a novel neuromorphic vision sensor with deep learning neural networks for autonomous electric cars. More precisely, this work will be conducted with the Schanzer Racing Electric (SRE) team at the Technical University of Ingolstadt. SRE is a team of around 80 students, that design, develop and manufacture an all-electric racing car every year to compete in Formula Student Electric. The use of neuromorphic vision sensors together with deep neural networks for object detection is the innovation that the project proposes. The project was supported by nVidia through a GPU Research Grant.
</p>

<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/caxenie-patch-1/pics/SRd17.jpg" width="80%" height="80%" alt=""></p>
	    
<p>	    
Context
</p>
<p>
Autonomous driving is a highly discussed topic, but in order to operate autonomously, the car needs to sense its environment. Vision provides the most informative and structured modality capable of grounding perception in autonomous vehicles. In the last decades, classical computer vision algorithms were used to not only locate relevant objects in the scene, but also to classify them. But in recent years, major improvements were reached when first deep learning object detectors were developed.

In general, such object detectors use a convolutional feature extractor as their basis. Due to the multitude of feature extraction algorithms, there are numerous combinations of feature extractor and object detectors, which influences a system designer’s approach. One of the most interesting niches is the analysis of traffic scenarios. Such scenarios require fast computation of features and classification for decision making.
</p>
<p>	    
Our approach to object detection, recognition and decision making aims at “going away from frames”. Instead of using traditional RGB cameras we aim at utilizing dynamic vision sensors (DVS - https://inivation.com/dvs/). Dynamic vision sensors mimic basic characteristics of human visual processing (i.e. neuromorphic vision) and have created a new paradigm in vision research.

Similar to photoreceptors in the human retina, a single DVS pixel (receptor) can generate events in response to a change of detected illumination. Events encode dynamic features of the scene, e.g. moving objects, using a spatio-temporal set of events. Since DVS sensors drastically reduce redundant pixels (e.g. static background features) and encode objects in a frame-less fashion with high temporal resolution (about 1 μs), it is well suited for fast motion analyses and tracking. DVS are capable of operating in uncontrolled environments with varying lighting conditions because of their high dynamic range of operation (120 dB).
</p>

<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/caxenie-patch-1/pics/Autonomous System.jpg" width="80%" height="80%" alt=""></p>

<p>	    
As traffic situations yield fast detection and precise estimation, we plan to use such an event-based visual representation together with two convolutional networks proved to be suitable for the task. The two algorithms we plan to explore are the Single Shot Multibox Detector (SSMD), which got popular for its fast computation speed, and the Faster Region-Based Convolutional Neural Network (Faster RCNN), which is known to be a slow but performant detector.
</p>
<p>	    
Motivation
</p>
<p>
The project tries to set a fundamental exploratory work, both in terms of sensory data for environment perception and also neural network architectures for the considered task. The experiments aim at evaluating also the points where better accuracy can only be obtained by sacrificing computation time. The two architecture we chose are opposite. The first one is the SSMD network with Inception V2 as a feature extractor. This network has a low computation time with acceptable accuracy. The correspondent network is the Faster RCNN with ResNet-101 as its feature extractor. Its accuracy is one of the highest, whereas the computation time is relatively slow.

Whereas features are common for frame-based computer vision problems, no solution exists yet to determine unique features in event streams. This is the first step towards more complex algorithms operating on the sparse event-stream. The possibility to create unique filter responses gives rise to the notion of temporal features. This opens the exploratory work we envision in this project, to investigate the use of SSMD and Faster RCNN networks using event-based input in a natively parallel processing pipeline.
</p>
<p>
Preliminary results
</p>
<p>
The initial step was carried in training a single shot detector (mobilenet) for the cone detection, a stage in the preparation the Formula Electric competition. Experiments were carried on an nVidia GTX1080Ti GPU using TensorRT. The performance evaluation is shown in the following diagrams.	
</p>

<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/caxenie-patch-1/pics/setup.png" width="80%" height="80%" alt=""></p>

<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/caxenie-patch-1/pics/tensorboard.png" width="100%" height="100%" alt=""></p>
		
	      
<h3>
<a id="adaptive sensorimotor control" class="anchor" href="#adaptive-senosorimotor-control" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adaptive Neuromorphic Sensorimotor Control </h3>

<p> Efficient sensorimotor processing is inherently driven by physical real-world constraints that an acting agent faces in its environment. Sensory streams contain certain statistical dependencies determined by the structure of the world, which impose constraints on a system’s sensorimotor affordances. </p>
<p> This limits the number of possible sensory information patterns and plausible motor actions. Learning mechanisms allow the system to extract the underlying correlations in sensorimotor streams.</p>
<p> This research direction focused on the exploration of sensorimotor learning paradigms for embedding adaptive behaviors in robotic system and demonstrate flexible control systems using neuromorphic hardware and neural-based adaptive control. I employed large-scale neural networks for gathering and processing complex sensory information, learning sensorimotor contingencies, and providing adaptive responses. </p>
<p> To investigate the properties of such systems, I developed flexible embodied robot platforms and integrate them within a rich tool suite for specifying neural algorithms that can be implemented in embedded neuromorphic hardware.</p>

<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/master/pics/neurobot1.png" width="80%" height="80%" alt=""></p>
		
<p> The mobile manipulator I developed at NST for adaptive sensorimotor systems consists of an omni-directional (holonomic) mobile manipulation platform with embedded low-level motor control and multimodal sensors. </p> 
<p> The on-board micro-controller receives desired commands via WiFi and continuously adapts the platform's velocity controller. The robot’s integrated sensors include wheel encoders for estimating odometry, a 9DoF inertial measurement unit, a proximity bump-sensor ring and three event-based embedded dynamic vision sensors (eDVS) for visual input.</p>

<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/master/pics/neurobot2.png" width="80%" height="80%" alt=""></p>
		
<p>The mobile platform carries an optional 6 axis robotic arm with a reach of >40cm. This robotic arm is composed of a set of links connected together by revolute joints and allows lifting objects of up to 800 grams. The mobile platform contains an on-board battery of 360 Wh, which allows autonomous operation for well above 5h. </p>
<h3>	
<a id="publications" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Publications</h3>

<h3>
<a id="peer-reviewed-journal-papers" class="anchor" href="#peer-reviewed-journal-papers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Peer reviewed journal papers</h3>

<p>F. Mirus, C. Axenie, T. C. Stewart, J. Conradt, Neuromorphic Sensorimotor Adaptation for Robotic Mobile Manipulation: From Sensing to Behaviour, Cognitive Systems Research, 2018. </p>
<p> I. Sugiarto, C. Axenie, J. Conradt, FPGA-based Hardware Accelerator for an Embedded Factor Graph with Configurable Optimization, Journal of Circuits, Systems and Computers, 2018. </p>
	    
<h3>
<a id="synthesis-of-distributed-cognitive-systems---learning-and-development-of-multisensory-integration" class="anchor" href="#synthesis-of-distributed-cognitive-systems---learning-and-development-of-multisensory-integration" aria-hidden="true"><span class="octicon octicon-link"></span></a>Synthesis of Distributed Cognitive Systems - Learning and Development of Multisensory Integration</h3>

<p>My research interest is in developing sensor fusion mechanisms for robotic applications. In order to extend the interacting areas framework a second direction in my research focuses on learning and development mechanisms.</p>

<p>Human perception improves through exposure to the environment. A wealth of sensory streams which provide a rich experience continuously refine the internal representations of the environment and own state. Furthermore, these representations determine more precise motor planning.</p>

<p>An essential component in motor planning and navigation, in both real and artificial systems, is egomotion estimation. Given the multimodal nature of the sensory cues, learning crossmodal correlations improves the precision and flexibility of motion estimates.</p>

<p>During development, the biological nervous system must constantly combine various sources of information and moreover track and anticipate changes in one or more of the cues. Furthermore, the adaptive development of the functional organisation of the cortical areas seems to depend strongly on the available sensory inputs, which gradually sharpen their response, given the constraints imposed by the cross-sensory relations.</p>

<p>Learning processes which take place during the development of a biological nervous system enable it to extract mappings between external stimuli and its internal state. Precise egomotion estimation is essential to keep these external and internal cues coherent given the rich multisensory environment. In this work we present a learning model which, given various sensory inputs, converges to a state providing a coherent representation of the sensory space and the cross-sensory relations.</p>

<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/master/pics/figure1.png" width="100%" height="100%" alt="Figure 1"></p>

<p>The model is based on Self-Organizing-Maps and Hebbian learning (see Figure 1) using sparse population coded representations of sensory data. The SOM is used to represent the sensory data, while the Hebbian linkage extracts the coactivation pattern given the input modalitites eliciting peaks of activity in the neural populations. The model was able to learn the intrinsic sensory data statistics without any prior knowledge (see Figure 2).</p>

<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/master/pics/figure2.png" width="100%" height="100%" alt="Figure 2"></p>

<p>The developed model, implemented for 3D egomotion estimation on a quadrotor, provides precise estimates for roll, pitch and yaw angles (setup depicted in Figure 3a, b).</p>

<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/master/pics/figure3.png" alt="Figure 3"></p>

<p>Given relatively complex and multimodal scenarios in which robotic systems operate, with noisy and partially observable environment features, the capability to precisely and timely extract estimates of egomotion critically influences the set of possible actions.</p>

<p>Utilising simple and computationally effective mechanisms, the proposed model is able to learn the intrinsic correlational structure of sensory data and provide more precise estimates of egomotion (see Figure 4a, b).</p>

<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/master/pics/figure4.png" alt="Figure 4"></p>

<p>Moreover, by learning the sensory data statistics and distribution, the model is able to judiciously allocate resources for efficient representation and computation without any prior assumptions and simplifications. Alleviating the need for tedious design and parametrisation, it provides a flexible and robust approach to multisensory fusion, making it a promising candidate for robotic applications.</p>

<h3>
<a id="publications" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Publications</h3>

<h3>
<a id="peer-reviewed-journal-papers" class="anchor" href="#peer-reviewed-journal-papers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Peer reviewed journal papers</h3>
		
<p>C. Axenie, C. Richter, J. Conradt, A Self-Synthesis Approach to Perceptual Learning for Multisensory Fusion in Robotics, Sensors Journal, 2016 (<a href="http://www.mdpi.com/1424-8220/16/10/1751">PDF</a>) </p>

<h3>
<a id="peer-reviewed-conference-papers" class="anchor" href="#peer-reviewed-conference-papers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Peer reviewed conference papers</h3>

<p>C. Axenie, J. Conradt, Learning Sensory Correlations for 3D Egomotion Estimation, International Conf. on Biomimetics and Biohybrid Systems, 2015</p>

<h3>
<a id="poster-presentations" class="anchor" href="#poster-presentations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Poster presentations</h3>

<p>C. Axenie, J. Conradt, A model for development and emergence in multisensory integration, Bernstein Conference on Computational Neuroscience, Göttingen, 2014. (<a href="http://www.nst.ei.tum.de/fileadmin/w00bqs/www/pdf/papers_axenie/2014_BCCN_Axenie_Conradt.pdf">PDF</a>)</p>

<hr>

<h3>
<a id="synthesis-of-distributed-cognitive-systems---interacting-cortical-maps-for-environmental-interpretation" class="anchor" href="#synthesis-of-distributed-cognitive-systems---interacting-cortical-maps-for-environmental-interpretation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Synthesis of Distributed Cognitive Systems - Interacting Cortical Maps for Environmental Interpretation</h3>

<p>The core focus of my research interest is in developing sensor fusion mechanisms for robotic applications. These mechanisms enable a robot to obtain a consistent and global percept of its environment using available sensors by learning correlations between them in a distributed processing scheme inspired by cortical mechanisms.</p>

<p>Environmental interaction is a significant aspect in the life of every physical entity, which allows the updating of its internal state and acquiring new behaviors. Such interaction is performed by repeated iterations of a perception-cognition-action cycle, in which the entity acquires and memorizes relevant information from the noisily and partially observable environment, to develop a set of applicable behaviors (see Figure 5). </p>

<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/master/pics/figure5.png" alt="Figure 5"></p>

<p>This recently started research project is in the area of mobile robotics; and more specifically in explicit methods applicable for acquiring and maintaining such environmental representations. State-of-the-art implementations build upon probabilistic reasoning algorithms, which typically aim at optimal solutions with the cost of high processing requirements.</p>

<p>In this project I am developing an alternative, neurobiologically inspired method for real-time interpretation of sensory stimuli in mobile robotic systems: a distributed networked system with inter-merged information storage and processing that allows efficient parallel reasoning. This networked architecture will be comprised of interconnected heterogeneous software units, each encoding a different feature about the state of the environment that is represented by a local representation (see Figure 6). </p>

<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/master/pics/figure6.png" alt="Figure 6"></p>

<p>Such extracted pieces of environmental knowledge interact by mutual influence to ensure overall system coherence. A sample instantiation of the developed system focuses on mobile robot heading estimation (see Figure 7). In order to obtain a robust and unambiguous description of robot’s current orientation within its environment inertial, proprioceptive and visual cues are fused (see image). Given available sensory data, the network relaxes to a globally consistent estimate of the robot's heading angle and position.</p>

<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/master/pics/figure7.png" alt="Figure 7"></p>

<h3>
<a id="publications-1" class="anchor" href="#publications-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Publications</h3>

<h3>
<a id="journal-articles" class="anchor" href="#journal-articles" aria-hidden="true"><span class="octicon octicon-link"></span></a>Journal articles</h3>

<p>I. Sugiarto, C. Axenie, J. Conradt, From Adaptive Reasoning to Cognitive Factory: Bringing Cognitive Intelligence to Manufacturing Technology, International Journal of Industrial Research and Applied Engineering (2016) (<a href="http://jirae.petra.ac.id/index.php/jirae/article/view/19317">PDF</a>)</p>

<p>I. Susnea, C. Axenie, Cognitive Maps for Indirect Coordination of Intelligent Agents, Studies in Informatics and Control Vol. 24 (2015) (<a href="http://sic.ici.ro/?page_id=632">PDF</a>)</p>

<p>C. Axenie, J. Conradt, Cortically inspired sensor fusion network for mobile robot egomotion estimation, Robotics and Autonomous Systems (2014) (<a href="http://www.sciencedirect.com/science/article/pii/S0921889014003133">PDF</a>) </p>

<h3>
<a id="peer-reviewed-conference-papers-1" class="anchor" href="#peer-reviewed-conference-papers-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Peer reviewed conference papers</h3>

<p>C. Axenie, J. Conradt, Cortically Inspired Sensor Fusion Network for Mobile Robot Heading Estimation, International Conf. on Artificial Neural Networks (ICANN), 2013, pp. 240-47. (<a href="http://link.springer.com/chapter/10.1007/978-3-642-40728-4_30">PDF</a>) </p>

<h3>
<a id="poster-presentations-1" class="anchor" href="#poster-presentations-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Poster presentations</h3>

<p>C. Axenie, M. Firouzi, M.Mulas, J. Conradt, Multimodal sensor fusion network for mobile robot egomotion estimation, Bernstein Conference on Computational Neuroscience, Göttingen, 2014. (<a href="http://www.nst.ei.tum.de/fileadmin/w00bqs/www/pdf/papers_axenie/2014_BCCN_Axenie_Firouzi_Mulas_Conradt.pdf">PDF</a>) </p>

<p>M. Firouzi, C. Axenie, J. Conradt, Multi-sensory cue integration with reliability encoding, using Line Attractor Dynamics, searching for optimality, Bernstein Conference on Computational Neuroscience, Göttingen, 2014. (<a href="http://www.nst.ei.tum.de/fileadmin/w00bqs/www/pdf/papers_axenie/2014_BCCN_Firouzi_Axenie_Conradt.pdf">PDF</a>)</p>

<p>C. Axenie, M. Firouzi, J. Conradt, Multisensory Integration Network for Mobile Robot Self-motion Estimation, Bernstein Conference on Computational Neuroscience, Tübingen, 2013. (<a href="http://www.nst.ei.tum.de/fileadmin/w00bqs/www/pdf/papers_axenie/2013-SparksWS_Axenie_Firouzi_Conradt.pdf">PDF</a>)</p>

<p>C. Axenie, J. Conradt, Synthesis of Distributed Cognitive Systems: Interacting Maps for Sensor Fusion, Bernstein Conference on Computational Neuroscience, München, 2012. (<a href="http://www.nst.ei.tum.de/fileadmin/w00bqs/www/pdf/papers_axenie/2012-BCCN_Axenie_Conradt.pdf">PDF</a>)</p>

<hr>

<h3>
<a id="adaptive-nonlinear-control-algorithm-for-fault-tolerant-robot-navigation" class="anchor" href="#adaptive-nonlinear-control-algorithm-for-fault-tolerant-robot-navigation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adaptive Nonlinear Control Algorithm for Fault Tolerant Robot Navigation</h3>

<p>Today’s trends in control engineering and robotics are blending gradually into a slightly challenging area, the development of fault tolerant real-time applications. Hence, applications should timely deliver synchronized data-sets, minimize latency in their response and meet their performance specifications in the presence of disturbances. The fault tolerant behavior in mobile robots refers to the possibility to autonomously detect and identify faults as well as the capability to continue operating after a fault occurred. This work introduces a real-time distributed control application with fault tolerance capabilities for differential wheeled mobile robots (see Figure 8).</p>

<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/master/pics/figure8.png" alt="Figure 8"></p>

<iframe width="420" height="315" src="https://www.youtube.com/embed/UkgOAOTyQCQ" frameborder="0" allowfullscreen></iframe>

<p>Furthremore, the application was extended to introduce a novel implementation for limited sensor mobile robots environment mapping. The developed algorithm is a SLAM implementation. It uses real time data acquired from the sonar ring and uses this information to feed the mapping module for offline mapping (see Figure 9). </p>

<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/master/pics/figure9.png" alt="Figure 9"></p>

<p>The latter is running on top of the real time fault tolerant control application for mobile robot trajectory tracking operation (see Figures 10, 11). </p>

<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/master/pics/figure10.png" alt="Figure 10"></p>

<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/master/pics/figure11.png" alt="Figure 11"></p>

<p>Along the developed application, the mechanical design, electronics design and implementation were made as part of the BA and MA thesis projects.</p>

<h3>
<a id="publications-2" class="anchor" href="#publications-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Publications</h3>

<h3>
<a id="peer-reviewed-conference-papers-2" class="anchor" href="#peer-reviewed-conference-papers-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Peer reviewed conference papers</h3>

<p>Axenie Cristian, Stancu Alexandru, Zanoschi Aurelian, Pascalin Andrei, Perjeru Marius, Maftei Florentina, A Client-Server Based Real-Time Control Tool for Complex Distributed Systems, Proc. of 9th Real-Time Linux Workshop, Linz, Austria, November 2007 (<a href="https://www.osadl.org/fileadmin/events/rtlws-2007/Axenie.pdf">PDF</a>)</p>

<p>Cristian Axenie, “Mobile Robot Fault Tolerant Control. Introducing ARTEMIC.” In the 9th International Conference on Signal Processing, Robotics and Automation WSEAS Conference Proceedings Included in ISI/SCI Web of Science and Web of Knowledge, University of Cambridge, UK, February 2010 (<a href="http://www.wseas.us/e-library/conferences/2010/Cambridge/ISPRA/ISPRA-34.pdf">PDF</a>)</p>

<p>Cristian Axenie, “A New Approach in Mobile Robot Fault Tolerant Control. Minimizing Costs and Extending Functionality”, Included in ISI / SCI (Web of Science) WSEAS TRANSACTIONS ON SYSTEMS AND CONTROL 2010 (<a href="http://www.wseas.us/e-library/transactions/control/2010/89-464.pdf">PDF</a>)</p>

<p>Cristian Axenie, Cernega Daniela, “Mobile Robot Fault Tolerant Control”, IEEE/IACSIT ICIEE 2010 (International Conference on Information and Electronics Engineering), Shanghai, China, June 2010 (<a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5529958&amp;filter%3DAND%28p_IS_Number%3A5529772%29">PDF</a>)</p>

<p>Cristian Axenie, Cernega Daniela, “ Adaptive Sliding Mode Controller Design for Mobile Robot Fault Tolerant Control”, 19th  IEEE International Workshop on Robotics in Alpe-Adria-Danube Region, Budapest, Hungary, June 2010 (<a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=5524575&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D5524575">PDF</a>)</p>

<p>Cristian Axenie, Razvan Solea, “Real Time Control Design for Mobile Robot Fault Tolerant Control”, 2010 IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications, July 15-17, 2010, Qingdao, ShanDong, China (<a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=5552026&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F5547546%2F5551988%2F05552026.pdf%3Farnumber%3D5552026">PDF</a>) </p>
 	  
<hr>

<h2>Competitions / Hackathons</h2>
<h3>Merck Research "Future of AI Challenge" (August 2019)</h3>
IRENA (Invariant Representations Extraction in Neural Architectures)
Team NeuroTHIx codebase. 1st Place at Merck Future of AI Research Challenge.
<a href="https://app.ekipa.de/challenge/future-of-ai/about">https://app.ekipa.de/challenge/future-of-ai/about</a>

THI coverage:

<a href="https://www.thi.de/suche/news/news/thi-erfolgreich-in-ai-forschungswettbewerb">https://www.thi.de/suche/news/news/thi-erfolgreich-in-ai-forschungswettbewerb</a>
<p align="justify">Merck Research Challenge aimed to generate insights from various disciplines that can lead to progress towards an understanding of invariant representation - that are novel and not based on Deep Learning.</p>
<p align="justify">IRENA (Invariant Representations Extraction in Neural Architectures) is the approach that team NeuroTHIx developed. IRENA offers a computational layer for extracting sensory relations for rich visual scenes, withy learning, inference, de-noising and sensor fusion capabilities. The system is also capable, through its underlying unsupervised learning capabilities, to embed semantics and perform scene understanding.</p>
Using cortical maps as neural substrate for distributed representations of sensory streams, our system is able to learn its connectivity (i.e., structure) from the long-term evolution of sensory observations. This process mimics a typical development process where self-construction (connectivity learning), self-organization, and correlation extraction ensure a refined and stable representation and processing substrate. Following these principles, we propose a model based on Self-Organizing Maps (SOM) and Hebbian Learning (HL) as main ingredients for extracting underlying correlations in sensory data, the basis for subsequently extracting invariant representations.

&nbsp;

<a href="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/figure6bw.png"><img class="wp-image-26722 alignleft" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/figure6bw.png" alt="" width="343" height="334" /></a>

<a href="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/figure7bw.png">
<img class="wp-image-26725 aligncenter" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/figure7bw-1024x638.png" alt="" width="503" height="314" /></a>

&nbsp;

<hr />

<h3>University of Cambridge Hackathon - Hack Cambridge (January 2017)</h3>
&nbsp;

Microsoft Faculty Connection coverage (<a href="http://goo.gl/uPWGna">http://goo.gl/uPWGna</a>) for project demo at Hack Cambridge 2017, 28 – 29 January 2017, University of Cambridge with a Real-time Event-based Vision Monitoring and Notification System for Seniors and Elderly using Neural Networks.

It has been estimated that 33% of people age 65 will fall. At around 80, that increases to 50%. In case of a fall, seniors who receive help within an hour have a better rate of survival and, the faster help arrives, the less likely an injury will lead to hospitalization or the need to move into a long-term care facility. In such cases fast visual detection of abnormal motion patterns is crucial.

In this project we propose the use of a novel embedded Dynamic Vision Sensor (eDVS) for the task of classifying falls. Opposite from standard cameras which provide a time sequenced stream of frames, the eDVS provides only relative changes in a scene, given by individual events at the pixel level. Using this different encoding scheme the eDVS brings advantages over standard cameras. First, there is no redundancy in the data received from the sensor, only changes are reported. Second, as only events are considered the eDVS data rate is high. Third, the power consumption of the overall system is small, as just a low-end microcontroller is used to fetch events from the sensor and can ultimately run for long time periods in a battery powered setup. This project investigates how can we exploit the eDVS fast response time and low-redundancy in making decisions about elderly motion.

The computation back-end will be realized with a neural network classification to detect fall and filter outliers. The data will be provided from 2 stimuli (blinking LEDs at different frequencies) and will represent the actual position of the person wearing them. The changes in position of the stimuli will encode the possible positions corresponding to falls or normal cases.

We will use Microsoft Azure ML Studio to implement a MLP binary classifier for the 4 (2 stimuli x 2 Cartesian coordinates - (x,y) in the field of view) dimensional input. We labelled the data with Fall (F) and No Fall (NF).

&nbsp;

<a href="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/cambridge_hack.png"><img class="wp-image-24230 aligncenter" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/cambridge_hack.png" alt="" width="809" height="518" /></a>

<hr />

<h3>ANDRITZ Pioneers Hackathon (January 2017)</h3>
&nbsp;

Best innovation idea at the ANDRITZ Pioneers Hackaton innovating for the international technology group ANDRITZ. Developed an artificial neural learning agent for automation process productivity enhancement.

&nbsp;

<img class="wp-image-24227 aligncenter" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/andritz_hack.png" alt="" width="760" height="412" />

<hr />

<h3>Wellcome Trust Hack The Senses Hackathon (June 2016)</h3>
&nbsp;

WIRED UK coverage ( <a href="http://goo.gl/5yQ1Fn">http://goo.gl/5yQ1Fn</a> ) at the Hack the Senses hackathon in London: How to hack your senses: from 'seeing' sound to 'hair GPS': "Two-man team HearSee built a headband that taps into synaesthesia, translating changes in frame-less video to sound allowing blind people or those with a weak vision to see motion. Roboticist and neurologist Cristian Axenie assembled the hardware in mere minutes – attaching a pair of cameras and wires to a terrycloth headband."

&nbsp;

<hr />

<h3>Daimler FinTech Hackathon (April 2016)</h3>
&nbsp;

Awarded 1st prize (team) at the Daimler Financial Services Big Data Analytics Hackaton for the design of a neuro-fuzzy learning system for anomaly detection and user interaction in big data streams.

&nbsp;

<img class="wp-image-24215 aligncenter" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/daimler_hack.png" alt="" width="797" height="487" />

<hr />

<h3>Burda Hackdays (April 2016)</h3>
&nbsp;

Awarded special Microsoft Cognitive Technologies prize at the Burda Hackdays for the design of a neural learning system for inferring role assignments in working teams using psychometric data analytics.

<a href="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/burda_hack.png"><img class="wp-image-24212 aligncenter" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/burda_hack.png" alt="" width="781" height="446" /></a>

<hr />

<h3>Automotive Hackdays (March 2016)</h3>
&nbsp;

Awarded 1st prize (team) in the BMW Automotive Hackdays for the design of an inference system for driving profile learning and recommendation for skills improvement and predictive maintenance in car-sharing.

<a href="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/bmw_hack.png"><img class="wp-image-24209 aligncenter" src="https://audi-konfuzius-institut-ingolstadt.de/wp-content/uploads/2018/09/bmw_hack.png" alt="" width="774" height="395" /></a>


        </section>

        <aside id="sidebar">
		
	<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/master/pics/HERC.png" style="float: center; width: 100%; margin-right: 1%; margin-bottom: 0.5em;" alt=""></p>
		
	<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/master/pics/AKII.png" style="float: center; width: 100%; margin-right: 1%; margin-bottom: 0.5em;" alt=""></p>
	
	<p><img src="https://raw.githubusercontent.com/caxenie/neurorobotics.me_backend/master/pics/thi.png" style="float: center; width: 100%; margin-right: 1%; margin-bottom: 0.5em;" alt=""></p>
		
	<iframe src="https://githubbadge.appspot.com/caxenie" style="border: 0;height: 111px;width: 200px;overflow: hidden;" frameBorder="0"></iframe>

		
        <h2>Latest News</h2>
	<h3>June 2020</h3>
	<p> One new paper combining machine learning and mechanistic modelling for cancer growth curve extraction entitled "GLUECK: Growth pattern Learning for Unsupervised Extraction of Cancer Kinetics" by Cristian Axenie and Daria Kurz was accepted at ECML PKDD 2020, the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases that will take place in Ghent, Belgium, from the 14nd to the 18nd of September 2020.</p>
	<h3>May 2020</h3>
	<p> One new paper combining machine learning and mechanistic modeling of cancer entitled "PRINCESS: Prediction of Individual Breast Cancer Evolution to Surgical Size" by Cristian Axenie and Daria Kurz, was accepted at the IEEE 33rd International Symposium on Computer-Based Medical Systems (CBMS), Mayo Clinic, Rochester, MN, July 28 - 30, 2020 (Virtual Meeting).</p>
	<h3>February 2020</h3>
	<p> One poster abstract on Learning Personalized Virtual Reality Avatars for Chemotherapy-Induced Peripheral Neuropathy Rehabilitation in Breast Cancer by Daria Kurz and Cristian Axenie accepted and presented at 34th German Cancer Congress will take place from February 19 to 22, 2020, in Berlin.</p>
	<h3>December 2019</h3>
	<p> Two papers accepted at the IEEE ICMLA 2019: An Online Incremental Clustering Framework for Real-Time Predictive Analytics on Datastreams by Carlos, S.; Tudoran, R.; Bortoli, S.; Hassan, M. A. H.; Brasche, G.; and Axenie, and Dimensionality Reduction for Low-Latency High-Throughput Fraud Detection on Datastreams by Axenie, C.; Tudoran, R.; Bortoli, S.; Hassan, M. A. H.; Carlos, S.; and Brasche, G. In 2019 18th IEEE International Conference on Machine Learning and Applications (ICMLA), 2019. IEEE </p>
	<h3>August 2019</h3>
	<p>The PERSEUS (Platform for Enhanced Virtual Reality in Sport Exercise Understanding and Simulation) research project, which deals with the development of VR-based goalkeeping training, successfully received the financial support from the Federal Ministry for Economic Affairs and Energy – BMWi). AKII Microlab will design and develop AI algorithms for VR avatar-based reconstruction of the player’s movement, able to track and simulate the whole-body skeletal movement of goalkeepers using embedded glove sensors. The funding is part of the Central Innovation Program for SMEs (ZIM) and thus supports our research activities in the field of VR and AI.</p>
	<h3>August 2019</h3>
	<p> A team of two students from THI, Du Xiaorui (Masters, Computer Science), Yavuzhan Erdem (Bachelor, Mechatronics) lead by Dr. Cristian Axenie (the head of the Audi Konfuzius-Institut Ingolstadt Lab) have been awarded the 1st Prize in the prestigious European Merch AI Research Challenge. </p>
	<h3>July 2019</h3>
	<p> The International Conference on Bioinformatics and Biomedical Engineering (BIBE) is the annual flagship biomed conference of the IEEE and EMB. For the 2019 edition of BIBE, AKII Microlab has two papers accepted. C. Axenie, Armin Becher, Daria Kurz, Thomas Grauschopf, Meta-Learning for Avatar Kinematics Reconstruction in Virtual Reality Rehabilitation; and C. S. Sanchez, J. Baumbach, S. Smyth, C. Axenie, Fuzzy Inference System for Risk Evaluation in Gestational Diabetes Mellitus.</p>
	<h3>May 2019</h3>
	<p> Two new papers accepted at the 28th International Conference on Artificial Neural Networks, the annual flagship conference of the European Neural Network Society (ENNS): Neural Network 3D Body Pose Tracking and Prediction for Motion-to-Photon Latency Compensation in Distributed Virtual Reality - Sebastian Pohl, Armin Becher, Thomas Grauschopf, Cristian Axenie; NARPCA: Neural Accumulate-Retract PCA for Low-latency High-throughput Processing on Datastreams - Cristian Axenie, Radu Tudoran, Stefano Bortoli, Mohamad Al Hajj Hassan, Goetz Brasche. Well done team ! </p>
	<h3>February 2019</h3>
	<p>Invited talk at Lions Club Salzburg: AI and VR Applications.</p>
	<h3>Febrauary 2019</h3>
	<p>Organized the THI-SCUT: Artificial Reality Research and Cooperation Seminar. A cooperation between Technische Hochschule Ingolstadt (THI), South China University of Technology (SCUT) and Audi Konfuzius Institute Ingolstadt.</p>		
	<h3>September 2018</h3>
	<p>Invited talk at Lions Club Ubersee Cyber: AI and VR for the Future of Society.</p>	
	<h3>August 2018</h3>
	<p>Two new papers introducing an important study on a  Novel Approach to Measure Motion-To-Photon and Mouth-To-Ear Latency in Distributed Virtual Reality Systems (Becher, Angerer, Grauschopf)  and  our framework, VIRTOOAIR: VIrtual Reality TOOlbox for Avatar Intelligent Reconstruction (Becher, Axenie, Grauschopf), were accepted in two important workshops. The first paper will be published within the 2018 Gesellschaft für Informatik e.V. (GI) Workshop on Virtuelle Realität und Augmented Reality (VR/AR) and the second in the MULTIMODAL VIRTUAL & AUGMENTED REALITY WORKSHOP (MVAR 2018) within 2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) respectively.</p>	
	<h3>April 2018</h3>
	<p>Awarded a nVidia GPU Grant for a Neuromorphic Processing for Electric Autonomous Driving with Schanzer Racing Electric (SRE) team at Technical University of Ingolstadt.</p>			
	<h3>October 2017</h3>
	<p>Joined the AUDI Konfuzius Institute Ingolstadt to lead the AKII Microlab and apply AI and ML to VR. </p>			
	<h3>October 2017</h3>
	<p>Joined the Technical University of Ingolstadt as Lecturer in Artificial Intelligence and Machine Learning. </p>			
	<h3>April 2017</h3>
	<p>Joined the Huawei European Research Center as Senior Research Engineer to lead work on online Machine Learning for Big Data. </p>			
	<h3>March 2017</h3>
	<p>Invited talk at TEDx - Calea Domneasca 18.03.2017 (http://www.ted.com/tedx/events/21484), Theme: Dare to leave a mark in Galati, Romania. </p>			
	<h3>January 2017</h3>
	<p>Microsoft Faculty Connection coverage (goo.gl/uPWGna) for project demo at Hack Cambridge 2017, 28 – 29 January 2017, University of Cambridge with a Real-time Event-based Vision Monitoring and Notification System for Seniors and Elderly using Neural Networks.</p>	
	<h3>January 2017</h3>
	<p>Awarded a BayIntAn Fellowship (5000 EUR) by the Bavarian Research Alliance for establishing a cooperation on the development of a platform for neuromorphic models of sensorimotor adaptation with ETH Zurich, Switzerland and University of California, Irvine, US.</p>
	<h3>January 2017</h3>
	<p>Guest lecturer in Neural Computation at BaseCamp winter school in Wien. An immersive full-time program for prospective data scientists to deepen theoretical knowledge and enhance practical skills for data analytics.</p>
	<h3>January 2017</h3>
	<p>Best innovation idea at the ANDRITZ Pioneers Hackaton innovating for the international technology group ANDRITZ. Developed an artificial neural learning agent for automation process productivity enhancement.</p>
	<h3>July 2016</h3>
	<p>Awarded a BayIntAn Fellowship (10000EUR) by the Bavarian Research Alliance for establishing a cooperation on neurorobotics with University of Waterloo, Canada and the University of Manchester, UK.</p>
        <h3>June 2016</h3>
        WIRED UK coverage (http://goo.gl/5yQ1Fn) at the Hack the Senses hackathon in London:
	How to hack your senses: from 'seeing' sound to 'hair GPS':
	
	"Two-man team HearSee built a headband that taps into synaesthesia, translating changes in frameless video to sound allowing blind people or those with weak vision to see motion. Roboticist and neurologist Cristian Axenie assembled the hardware in mere minutes – attaching a pair of cameras and wires to a terrycloth headband."
      	<h3>April 2016</h3>
      	<p>Awarded 1st prize (team) at the Daimler Financial Services Big Data Analytics Hackaton for the design of a neurofuzzy learning system for anomaly detection and user interaction in big data streams.</p>
      	<h3>April 2016</h3>
      	<p>Awarded special Microsoft Cognitive Technologies prize at the Burda Hackdays for the design of a neural learning system for inferring role assignments in working teams using psychometric data analytics.</p>
      	<h3>April 2016</h3>
      	<p>Awarded PhD (Summa cum Laude) degree in Neuroscience and Robotics from the Technical University of Munich.</p>
      	<h3>March 2016</h3>
      	<p>Awarded 1st prize (team) in the BMW Automotive Hackdays for the design of an inference system for driving profile learning and recommendation for skills improvement and predictive maintenance in car-sharing.</p>
	<h3>July 2013</h3>
      	<p>Awarded the Convergent Science Network of Biomimetic and Biohybrid Systems Research Fellowship for the 2013 Telluride Neuromorphic Cognition Engineering Workshop.</p>
	<h3>May 2013</h3>
      	<p>Awarded the Convergent Science Network of Biomimetic and Biohybrid Systems Research Fellowship for the 2013 CapoCaccia Cognitive Neuromorphic Engineering Workshop.</p>
	<h3>April 2013</h3>
      	<p>Awarded the Leonhard Lorenz-Stiftung Scholarship for novel ideas in research.</p>
	<h3>April 2012</h3>
      	<p>Awarded the Bavarian Elite Research Scholarship by the Bavarian State Ministry of Sciences, Research and the Arts.</p>
	<h3>September 2009</h3>
      	<p>4th place at the National IBM “Best Linux Application” programming contest for work in robot fault-tolerant control using custom embedded Linux.</p>
	<h3>May 2009</h3>
      	<p>Best paper award at the 13th International Scientific Sessions ZTS, UPT (Romania) for work on nonlinear control for mobile robots.</p>
	<h3>September 2008</h3>
      	<p>Designer certificate on dependable embedded systems analysis and design from University of Lucern (Switzerland).</p>
	<h3>June 2008</h3>
      	<p>Invited talk at International Student Scientific Sessions of the Military Technical Academy (Romania) for work on adaptive systems design.</p>
	<h3>November 2007</h3>
      	<p>Invited presentation at the 9th Real-Time Linux Workshop (Austria) for work on real-time control in distributed systems.</p>
      	
        <iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="http://colmdoyle.github.io/gh-activity/gh-activity.html?user=caxenie&type=user" width="360" height="600">Current projects</iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=glueck&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=princess&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=tucann&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=chimera&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=irena&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=fusion-maps-analyzer&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=fusion-maps-population-code&type=repo" width="360" height="450"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=sensory-projections-som&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=dev-sensor-fusion-net&type=repo" width="360" height="450"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=developing-fusion-network-fast&type=repo" width="360" height="450"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=corr-learn-som-quadrotor&type=repo" width="360" height="450"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=fusion-maps-analyzer-matlab&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=corr-learn-som-fast&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=fuzzy-ctrl-demo&type=repo" width="360" height="250"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=sharp-learning&type=repo" width="360" height="250"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=unsupervised-relation-learning&type=repo" width="360" height="250"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=sigma-pi-som-learning&type=repo" width="360" height="250"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=ibfn-net-cue-integration&type=repo" width="360" height="250"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=hopfield-memory-demo&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=hopfield-distort-demo&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=gas-tsp-demo&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=gas-antenna-demo&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=dynamic-association-net&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=gas-opt-demo&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=ip-camera-overhead-tracker&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=spartan6-linux-dev&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=embedded-ser2eth-converter&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=msp430-android-gyro-acc-tester&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=artemic&type=repo" width="360" height="350"></iframe>
<iframe allowtransparency="true" frameborder="0" scrolling="yes" seamless="seamless" src="https://caxenie.github.io/github-state-widget.html?user=caxenie&repo=nst_drone_full_infrastructure&type=repo" width="360" height="350"></iframe>
        </aside>
      </div>
    </div>

  	<script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-65891127-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>
          
  </body>
</html>

